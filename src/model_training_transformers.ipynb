{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a67cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MestradoModelo\\ModelTraining\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports \n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing              import LabelEncoder\n",
    "from sklearn.model_selection            import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble                   import RandomForestClassifier\n",
    "from sklearn.naive_bayes                import GaussianNB\n",
    "from sklearn.metrics                    import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text    import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.models            import Sequential\n",
    "from tensorflow.keras.layers            import Dense, Dropout, LSTM, Embedding\n",
    "\n",
    "from scipy.stats                        import friedmanchisquare, ttest_rel\n",
    "from gensim.models                      import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers.modeling_tf_utils as mfu\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b450898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the user is on the login page</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the user has entered a valid username</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the user has entered a valid password</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the user is logged in</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the user is on the homepage</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     step        label\n",
       "0           the user is on the login page  Precondição\n",
       "1   the user has entered a valid username  Precondição\n",
       "2   the user has entered a valid password  Precondição\n",
       "3                   the user is logged in  Precondição\n",
       "4             the user is on the homepage  Precondição"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ler_arquivo_txt(path):\n",
    "    return Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "given = ler_arquivo_txt(\"../data/GivenSteps.txt\")\n",
    "when = ler_arquivo_txt(\"../data/WhenSteps.txt\")\n",
    "then = ler_arquivo_txt(\"../data/ThenSteps.txt\")\n",
    "\n",
    "steps  = given + when + then\n",
    "labels = [\"Precondição\"]*len(given) + [\"Ação\"]*len(when) + [\"Resultado\"]*len(then)\n",
    "\n",
    "df = pd.DataFrame({\"step\": steps, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4d988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"y\"] = le.fit_transform(df[\"label\"])\n",
    "tokenized = [s.split() for s in df[\"step\"]]\n",
    "\n",
    "w2v = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df[\"step\"]).toarray()\n",
    "\n",
    "def get_features(X_tfidf, tok, w2v):\n",
    "    feats = np.zeros((len(tok), X_tfidf.shape[1] + w2v.vector_size))\n",
    "    for i, tks in enumerate(tok):\n",
    "        vec = np.mean([w2v.wv[w] for w in tks if w in w2v.wv], axis=0)\n",
    "        feats[i] = np.hstack((X_tfidf[i], vec))\n",
    "    return feats\n",
    "\n",
    "X = get_features(X_tfidf, tokenized, w2v)\n",
    "y = df[\"y\"].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3ed8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "def create_mlp_model(input_dim, num_classes):\n",
    "    neurons = 143\n",
    "    dropout_rate = 0.77\n",
    "    activation = 'tanh'\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons // 2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Long Short‑Term Memory (LSTM)\n",
    "def create_lstm_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=64, input_length=input_dim))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Random Forest\n",
    "def create_rf_model(n_estimators=50, random_state=42):\n",
    "    return RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "def create_nb_model():\n",
    "    return GaussianNB()\n",
    "\n",
    "# XGBoost\n",
    "def create_xgb_model():\n",
    "    return XGBClassifier(\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "        n_jobs=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77004fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Prepara o tokenizer e o modelo\n",
    "le_tf      = LabelEncoder().fit(df[\"label\"])\n",
    "labels_tf  = le_tf.transform(df[\"label\"])\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encodings  = tokenizer_(\n",
    "    df[\"step\"].tolist(),\n",
    "    truncation=True, padding=True,\n",
    "    max_length=64, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# 6.2 Monta tf.data.Dataset\n",
    "dataset_bert = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encodings),\n",
    "    labels_tf\n",
    ")).shuffle(len(df)).batch(16)\n",
    "\n",
    "# 6.3 Carrega e compila o modelo TF\n",
    "model_bert = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(le_tf.classes_)\n",
    ")\n",
    "model_bert.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f284af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "94/94 [==============================] - 133s 1s/step - loss: 0.3603 - accuracy: 0.9020\n",
      "Epoch 2/3\n",
      "94/94 [==============================] - 116s 1s/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "94/94 [==============================] - 125s 1s/step - loss: 0.0074 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21006bc3290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redireciona o keras dentro do transformers para o tf.keras\n",
    "mfu.keras = tf.keras\n",
    "mfu.keras.utils = tf.keras.utils\n",
    "\n",
    "# 6.4 Fine‑tuning\n",
    "model_bert.fit(dataset_bert, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2e411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) MLP\n",
    "mlp_model = create_mlp_model(X.shape[1], len(np.unique(y)))\n",
    "mlp_model.fit(X, y, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# 2) LSTM\n",
    "lstm_model = create_lstm_model(X.shape[1], len(np.unique(y)))\n",
    "lstm_model.fit(X, y, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# 3) Random Forest\n",
    "rf_model = create_rf_model()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# 4) GaussianNB\n",
    "gnb_model = create_nb_model()\n",
    "gnb_model.fit(X, y)\n",
    "\n",
    "# 5) XGBoost\n",
    "xgb_model = create_xgb_model()\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "\n",
    "# Guarde tudo num dicionário\n",
    "models = {\n",
    "    'MLP':  mlp_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'RF':   rf_model,\n",
    "    'GNB':  gnb_model,\n",
    "    'XGB':  xgb_model,\n",
    "    'BERT': model_bert\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124d1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_step_all_models(step, models, tfidf, w2v, le, tokenizer, le_tf):\n",
    "    # Extrai features TF-IDF + Word2Vec\n",
    "    vec_tfidf = tfidf.transform([step]).toarray()[0]\n",
    "    vec_w2v   = np.mean(\n",
    "        [w2v.wv[t] for t in step.split() if t in w2v.wv],\n",
    "        axis=0\n",
    "    ) if any(t in w2v.wv for t in step.split()) else np.zeros(w2v.vector_size)\n",
    "    X_vec = np.hstack([vec_tfidf, vec_w2v]).reshape(1, -1)\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name == 'BERT':\n",
    "            inputs = tokenizer_(step, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "            logits = model(inputs).logits\n",
    "            pred   = tf.argmax(logits, axis=1).numpy()[0]\n",
    "            results[name] = le_tf.inverse_transform([pred])[0]\n",
    "        else:\n",
    "            pred = model.predict(X_vec)\n",
    "            if isinstance(pred, np.ndarray) and pred.ndim == 2:\n",
    "                pred = pred.argmax(axis=1)[0]\n",
    "            results[name] = le.inverse_transform([int(pred)])[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f94889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 415ms/step\n",
      "MLP -> Precondição\n",
      "LSTM -> Resultado\n",
      "RF -> Ação\n",
      "GNB -> Ação\n",
      "XGB -> Ação\n",
      "BERT -> Resultado\n"
     ]
    }
   ],
   "source": [
    "step = \"\"\n",
    "\n",
    "predicoes = classify_step_all_models(\n",
    "    step,        # string a classificar\n",
    "    models,      # dict com 'MLP','LSTM','RF','GNB','XGB','BERT'\n",
    "    tfidf,       # seu TfidfVectorizer treinado\n",
    "    w2v,         # seu Word2Vec treinado\n",
    "    le,          # LabelEncoder dos modelos tradicionais\n",
    "    tokenizer_,  # AutoTokenizer do BERT\n",
    "    le_tf        # LabelEncoder usado só para o BERT\n",
    ")\n",
    "\n",
    "for nome, label in predicoes.items():\n",
    "    print(f\"{nome} -> {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
