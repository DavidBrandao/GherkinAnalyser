{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e46bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MestradoModelo\\ModelTraining\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports \n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing              import LabelEncoder\n",
    "from sklearn.model_selection            import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble                   import RandomForestClassifier\n",
    "from sklearn.naive_bayes                import GaussianNB\n",
    "from sklearn.metrics                    import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text    import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.models            import Sequential\n",
    "from tensorflow.keras.layers            import Dense, Dropout, LSTM, Embedding\n",
    "\n",
    "from scipy.stats                        import friedmanchisquare, ttest_rel\n",
    "from gensim.models                      import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers.modeling_tf_utils as mfu\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a85c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the user is on the login page</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the user has entered a valid username</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the user has entered a valid password</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the user is logged in</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the user is on the homepage</td>\n",
       "      <td>Precondição</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     step        label\n",
       "0           the user is on the login page  Precondição\n",
       "1   the user has entered a valid username  Precondição\n",
       "2   the user has entered a valid password  Precondição\n",
       "3                   the user is logged in  Precondição\n",
       "4             the user is on the homepage  Precondição"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ler_arquivo_txt(path):\n",
    "    return Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "given = ler_arquivo_txt(\"../data/GivenSteps.txt\")\n",
    "when = ler_arquivo_txt(\"../data/WhenSteps.txt\")\n",
    "then = ler_arquivo_txt(\"../data/ThenSteps.txt\")\n",
    "\n",
    "steps  = given + when + then\n",
    "labels = [\"Precondição\"]*len(given) + [\"Ação\"]*len(when) + [\"Resultado\"]*len(then)\n",
    "\n",
    "df = pd.DataFrame({\"step\": steps, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b164304",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"y\"] = le.fit_transform(df[\"label\"])\n",
    "tokenized = [s.split() for s in df[\"step\"]]\n",
    "\n",
    "w2v = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df[\"step\"]).toarray()\n",
    "\n",
    "def get_features(X_tfidf, tok, w2v):\n",
    "    feats = np.zeros((len(tok), X_tfidf.shape[1] + w2v.vector_size))\n",
    "    for i, tks in enumerate(tok):\n",
    "        vec = np.mean([w2v.wv[w] for w in tks if w in w2v.wv], axis=0)\n",
    "        feats[i] = np.hstack((X_tfidf[i], vec))\n",
    "    return feats\n",
    "\n",
    "X = get_features(X_tfidf, tokenized, w2v)\n",
    "y = df[\"y\"].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89e13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "def create_mlp_model(input_dim, num_classes):\n",
    "    neurons = 143\n",
    "    dropout_rate = 0.77\n",
    "    activation = 'tanh'\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons // 2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Long Short‑Term Memory (LSTM)\n",
    "def create_lstm_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=64, input_length=input_dim))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Random Forest\n",
    "def create_rf_model(n_estimators=50, random_state=42):\n",
    "    return RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "def create_nb_model():\n",
    "    return GaussianNB()\n",
    "\n",
    "# XGBoost\n",
    "def create_xgb_model():\n",
    "    return XGBClassifier(\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "        n_jobs=4\n",
    "    )\n",
    "\n",
    "# BERT\n",
    "\n",
    "# 6.1 Prepara o tokenizer e o modelo\n",
    "le_tf      = LabelEncoder().fit(df[\"label\"])\n",
    "labels_tf  = le_tf.transform(df[\"label\"])\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encodings  = tokenizer_(\n",
    "    df[\"step\"].tolist(),\n",
    "    truncation=True, padding=True,\n",
    "    max_length=64, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# 6.2 Monta tf.data.Dataset\n",
    "dataset_bert = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encodings),\n",
    "    labels_tf\n",
    ")).shuffle(len(df)).batch(16)\n",
    "\n",
    "# 6.3 Carrega e compila o modelo TF\n",
    "model_bert = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(le_tf.classes_)\n",
    ")\n",
    "model_bert.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc02990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "94/94 [==============================] - 146s 1s/step - loss: 0.3049 - accuracy: 0.9240\n",
      "Epoch 2/3\n",
      "94/94 [==============================] - 121s 1s/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "94/94 [==============================] - 119s 1s/step - loss: 0.0060 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x253ce017550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) MLP\n",
    "mlp_model = create_mlp_model(X.shape[1], len(np.unique(y)))\n",
    "mlp_model.fit(X, y, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# 2) LSTM\n",
    "lstm_model = create_lstm_model(X.shape[1], len(np.unique(y)))\n",
    "lstm_model.fit(X, y, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# 3) Random Forest\n",
    "rf_model = create_rf_model()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# 4) GaussianNB\n",
    "gnb_model = create_nb_model()\n",
    "gnb_model.fit(X, y)\n",
    "\n",
    "# 5) XGBoost\n",
    "xgb_model = create_xgb_model()\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "\n",
    "# Redireciona o keras dentro do transformers para o tf.keras\n",
    "mfu.keras = tf.keras\n",
    "mfu.keras.utils = tf.keras.utils\n",
    "\n",
    "# 6.4 Fine‑tuning\n",
    "model_bert.fit(dataset_bert, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90559c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarde tudo num dicionário\n",
    "models = {\n",
    "    'MLP':  mlp_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'RF':   rf_model,\n",
    "    'GNB':  gnb_model,\n",
    "    'XGB':  xgb_model,\n",
    "    'BERT': model_bert\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9397adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_step_all_models(step, models, tfidf, w2v, le, tokenizer, le_tf):\n",
    "    # Extrai features TF-IDF + Word2Vec\n",
    "    vec_tfidf = tfidf.transform([step]).toarray()[0]\n",
    "    vec_w2v   = np.mean(\n",
    "        [w2v.wv[t] for t in step.split() if t in w2v.wv],\n",
    "        axis=0\n",
    "    ) if any(t in w2v.wv for t in step.split()) else np.zeros(w2v.vector_size)\n",
    "    X_vec = np.hstack([vec_tfidf, vec_w2v]).reshape(1, -1)\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name == 'BERT':\n",
    "            inputs = tokenizer_(step, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "            logits = model(inputs).logits\n",
    "            pred   = tf.argmax(logits, axis=1).numpy()[0]\n",
    "            results[name] = le_tf.inverse_transform([pred])[0]\n",
    "        else:\n",
    "            pred = model.predict(X_vec)\n",
    "            if isinstance(pred, np.ndarray) and pred.ndim == 2:\n",
    "                pred = pred.argmax(axis=1)[0]\n",
    "            results[name] = le.inverse_transform([int(pred)])[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf44f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "MLP -> Resultado\n",
      "LSTM -> Ação\n",
      "RF -> Resultado\n",
      "GNB -> Resultado\n",
      "XGB -> Precondição\n",
      "BERT -> Resultado\n"
     ]
    }
   ],
   "source": [
    "step = \"the expected value is 3\"\n",
    "\n",
    "predicoes = classify_step_all_models(\n",
    "    step,        # string a classificar\n",
    "    models,      # dict com 'MLP','LSTM','RF','GNB','XGB','BERT'\n",
    "    tfidf,       # seu TfidfVectorizer treinado\n",
    "    w2v,         # seu Word2Vec treinado\n",
    "    le,          # LabelEncoder dos modelos tradicionais\n",
    "    tokenizer_,  # AutoTokenizer do BERT\n",
    "    le_tf        # LabelEncoder usado só para o BERT\n",
    ")\n",
    "\n",
    "for nome, label in predicoes.items():\n",
    "    print(f\"{nome} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bf324b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     step        label\n",
      "0                 Given the cart is empty  Precondição\n",
      "1  When the user adds an item to the cart         Ação\n",
      "2   Then the cart should contain the item    Resultado\n",
      "3            Given the user is logged out  Precondição\n",
      "4   When the user clicks the login button         Ação\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "MLP: acc=0.833, prec=0.889, rec=0.833, f1=0.822\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [0 1 1]\n",
      " [0 0 2]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "LSTM: acc=0.333, prec=0.111, rec=0.333, f1=0.167\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [2 0 0]\n",
      " [2 0 0]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "RF: acc=0.500, prec=0.467, rec=0.500, f1=0.413\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [2 0 0]\n",
      " [1 0 1]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "GNB: acc=0.667, prec=0.500, rec=0.667, f1=0.556\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [0 0 2]\n",
      " [0 0 2]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "XGB: acc=0.833, prec=0.889, rec=0.833, f1=0.822\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [0 2 0]\n",
      " [0 1 1]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "BERT: acc=0.833, prec=0.889, rec=0.833, f1=0.822\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [0 1 1]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "df_test = pd.read_csv(\"../data/real_steps.csv\")  \n",
    "print(df_test.head())\n",
    "y_true = df_test['label'].values\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = [ classify_step_all_models(step, models, tfidf, w2v, le, tokenizer_, le_tf)[name]\n",
    "               for step in df_test['step'] ]\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    prec  = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec   = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1    = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    print(f\"{name}: acc={acc:.3f}, prec={prec:.3f}, rec={rec:.3f}, f1={f1:.3f}\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "df_test = pd.read_csv(\"../data/real_steps.csv\")  \n",
    "print(df_test.head())\n",
    "y_true = df_test['label'].values\n",
    "\n",
    "# 1) Selecione apenas XGB e BERT:\n",
    "selected_models = {k: v for k, v in models.items() if k in ['XGB', 'BERT']}\n",
    "\n",
    "for name, model in selected_models.items():\n",
    "    print(f\"{name} → {model}\")\n",
    "    y_pred = [ classify_step_all_models(step, models, tfidf, w2v, le, tokenizer_, le_tf)[name]\n",
    "               for step in df_test['step'] ]\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    prec  = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec   = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1    = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    print(f\"{name}: acc={acc:.3f}, prec={prec:.3f}, rec={rec:.3f}, f1={f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1961c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
